{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic_Regression_Real_world .ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPOwlzZsPfv+i+q5qoF2dAX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"P46QzB6RieG4"},"source":["from google.colab import drive\r\n","drive.mount('/gdrive')\r\n","import numpy as np\r\n","import pandas as pd\r\n","import csv\r\n","from pandas import read_csv\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.linear_model import LogisticRegression\r\n","from matplotlib import pyplot as plt\r\n","from sklearn import preprocessing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sLYDipS4if0y"},"source":["def load_dataset_online_news():\r\n","  X = []\r\n","  y = []\r\n","  with open('/gdrive/MyDrive/OnlineNewsPopularity.csv', 'r') as online_news_sharing:\r\n","      online_news_sharing_reader = csv.reader(online_news_sharing)\r\n","      next(online_news_sharing_reader)\r\n","      for row in online_news_sharing_reader:\r\n","          row = [float(string) for string in row]\r\n","          X.append([row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[9],\\\r\n","              row[10], row[11], row[12], row[13], row[14], row[15], row[16], row[17], row[18], row[19],\\\r\n","              row[20], row[21], row[22], row[23], row[24], row[25], row[26], row[27], row[28], row[29],\\\r\n","              row[30], row[31], row[32], row[33], row[34], row[35], row[36], row[37], row[38], row[39],\\\r\n","              row[40], row[41], row[42], row[43], row[44], row[45], row[46], row[47], row[48], row[49],\\\r\n","              row[50], row[51], row[52], row[53], row[54], row[55], row[56], row[57]])\r\n","          y.append(row[58])\r\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\r\n","\r\n","  scaler = preprocessing.StandardScaler().fit(X_train)\r\n","  X_train = scaler.transform(X_train)\r\n","  X_test = scaler.transform(X_test)\r\n","\r\n","  return X_train, X_test, y_train, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6GAYrmPikzE"},"source":["def data_preprocessing(X_train, X_test, y_train, y_test, threshold, left_trunc_point = float(\"-inf\"), right_trunc_point = float(\"inf\")):\r\n","  X_train_trunc = []\r\n","  y_train_trunc = []\r\n","\r\n","  # Truncating training set\r\n","  for num in range(len(y_train)):\r\n","    if y_train[num] > left_trunc_point and y_train[num] < right_trunc_point:\r\n","      X_train_trunc.append(X_train[num])\r\n","      # Labeling data\r\n","      y_train_trunc.append(int(y_train[num] > threshold))\r\n","\r\n","  X_train_trunc = np.array(X_train_trunc)\r\n"," \r\n","  # Adding a column for bias\r\n","  column_ones = np.ones(X_train_trunc.shape[0])\r\n","  X_train_trunc = np.column_stack((column_ones, X_train_trunc))\r\n","\r\n","  # Labeling data on the untruncated testset \r\n","  y_test_processed = []\r\n","  X_test_processed = None\r\n","  for num in range(len(y_test)):\r\n","    y_test_processed.append(int(y_test[num] > threshold))\r\n","  X_test_processed = np.array(X_test)\r\n","  column_ones = np.ones(X_test_processed.shape[0])\r\n","  X_test_processed = np.column_stack((column_ones, X_test_processed))\r\n","\r\n","  return X_train_trunc, y_train_trunc, X_test_processed, y_test_processed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bAFSNPJjisc9"},"source":["def sigmoid(x):\r\n","  return 1 / (np.exp(-x) + 1)\r\n","\r\n","def cost_func(x, y, theta):\r\n","  pred = sigmoid(np.dot(x, theta))\r\n","  return -(y.T * np.log(pred) + (1 - y.T) * np.log(1 - pred)).sum() / len(pred)\r\n","\r\n","def compute_params(theta, x, left_trunc = float(\"-inf\"), right_trunc = float(\"inf\")):\r\n","  upper_lim = right_trunc - np.dot(theta, x)\r\n","  lower_lim = left_trunc - np.dot(theta, x)\r\n","  exp_noise = sigmoid(upper_lim) + sigmoid(lower_lim)\r\n","  return exp_noise, upper_lim, lower_lim\r\n","\r\n","def gradient(theta, x, y, left_trunc = float(\"-inf\"), right_trunc = float(\"inf\")):\r\n","  exp_noise, upper_lim, lower_lim = compute_params(theta, x, left_trunc, right_trunc)\r\n","\r\n","  if y == 1:\r\n","\r\n","    return np.dot((sigmoid(upper_lim) + sigmoid(np.max((lower_lim, -np.dot(theta, x)))) - exp_noise), x) \r\n","  else:\r\n","\r\n","    return np.dot((sigmoid(lower_lim) + sigmoid(np.min((upper_lim, -np.dot(theta, x)))) - exp_noise), x)\r\n","\r\n","def left_trunc_percentile(y_train, y_train_trunc):\r\n","  p = 1-(len(y_train_trunc)/len(y_train))\r\n","  percentile = -np.log((1-p)/p)\r\n","  return percentile\r\n","\r\n","\r\n","def test_accuracy(theta_cur, X_test_processed, y_test_processed, left_trunc_cur):\r\n","  y_pred = []\r\n","  \r\n","  for num in range(len(X_test_processed)):\r\n","    exp_noise, _, _ = compute_params(theta_cur, X_test_processed[num], left_trunc_cur)\r\n","    y_pred.append(sigmoid(np.dot(theta_cur, X_test_processed[num])+ 0.5 * exp_noise))\r\n","  y_pred = np.array(y_pred)\r\n","  pointwise_diff = abs((y_pred>0.5).astype(int) - y_test_processed)\r\n","  acc = 1-np.sum(pointwise_diff)/len(pointwise_diff)\r\n","  return acc\r\n","\r\n","def Logistic_SGD(x, y, max_iter = 500, learning_rate = 0.0003, left_trunc = float(\"-inf\"), right_trunc = float(\"inf\")):\r\n","  theta = np.zeros_like(x[0])\r\n","  theta = np.random.rand(theta.shape[0])\r\n","  x = np.array(x)\r\n","  y = np.array(y)\r\n","  bb = 0\r\n","  acc_prev = 0\r\n","  max_acc = 0\r\n","  lr = learning_rate\r\n","\r\n","  for iter in range(max_iter):\r\n","    state=np.random.get_state()\r\n","    np.random.shuffle(x)\r\n","    np.random.set_state(state)\r\n","    np.random.shuffle(y)\r\n","    if iter >= 20:\r\n","      lr = lr * 20/iter \r\n","    for index in range(len(x)):\r\n","      G = gradient(theta, x[index], y[index], left_trunc, right_trunc)\r\n","      theta = theta + lr * G\r\n","\r\n","  return theta"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CSRo0Eai9Da"},"source":["SGD_acc = []\r\n","tradition_acc = []\r\n","for i in range(7):\r\n","  threshold = 10000\r\n","  left_trunc_point = i*1000+1000\r\n","  X_train, X_test, y_train, y_test = load_dataset_online_news()\r\n","  X_train_trunc, y_train_trunc, X_test_processed, y_test_processed = data_preprocessing(X_train, X_test, y_train, y_test, threshold, left_trunc_point)\r\n","  l_trunc = left_trunc_percentile(y_train, y_train_trunc)\r\n","  max_iter = 100\r\n","  lr = 0.0003\r\n","  Theta = Logistic_SGD(X_train_trunc, y_train_trunc, max_iter = max_iter, learning_rate = lr, left_trunc = l_trunc)\r\n","  print(Theta)\r\n","  print(test_accuracy(Theta, X_test_processed, y_test_processed, l_trunc))\r\n","  SGD_acc.append(test_accuracy(Theta, X_test_processed, y_test_processed, l_trunc))\r\n","\r\n","  logi_model = LogisticRegression()\r\n","  pred = logi_model.fit(X_train_trunc, y_train_trunc).predict(X_test_processed)\r\n","  print(1-np.sum(abs(pred-y_test_processed))/len(X_test_processed))\r\n","\r\n","  tradition_acc.append(1-np.sum(abs(pred-y_test_processed))/len(X_test_processed))\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eN9wTRHCjJyN"},"source":["x_axis = np.arange(7)*1000+1000\r\n","y1 = SGD_acc\r\n","y2 = tradition_acc\r\n","plt.plot(x_axis, y1, label='Truncated Logistic Regression', color = 'red', marker = 's')\r\n","plt.plot(x_axis, y2, label='Standard Logistic Regression', color = 'blue', marker = 'o')\r\n","plt.xlabel('Truncation Parameter C',fontsize=14)\r\n","plt.ylabel(r'Testset Accuracy',fontsize=14)\r\n","plt.grid(alpha=0.4,linestyle=':')\r\n","plt.legend()\r\n","\r\n","plt.show()"],"execution_count":null,"outputs":[]}]}